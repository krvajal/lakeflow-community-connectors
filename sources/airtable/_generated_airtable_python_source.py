# ==============================================================================
# Merged Lakeflow Source: airtable
# ==============================================================================
# This file is auto-generated by scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime
from decimal import Decimal
from typing import (
    Any,
    Dict,
    Iterator,
    List,
    Optional,
    Tuple,
)
import json
import time

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None
        # Handle complex types
        if isinstance(field_type, StructType):
            # Validate input for StructType
            if not isinstance(value, dict):
                raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
            # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
            if value == {}:
                raise ValueError(
                    f"field in StructType cannot be an empty dict. Please assign None as the default value instead."
                )
            # For StructType, recursively parse fields into a Row
            field_dict = {}
            for field in field_type.fields:
                # When a field does not exist in the input:
                # 1. set it to None when schema marks it as nullable
                # 2. Otherwise, raise an error.
                if field.name in value:
                    field_dict[field.name] = parse_value(
                        value.get(field.name), field.dataType
                    )
                elif field.nullable:
                    field_dict[field.name] = None
                else:
                    raise ValueError(
                        f"Field {field.name} is not nullable but not found in the input"
                    )

            return Row(**field_dict)
        elif isinstance(field_type, ArrayType):
            # For ArrayType, parse each element in the array
            if not isinstance(value, list):
                # Handle edge case: single value that should be an array
                if field_type.containsNull:
                    # Try to convert to a single-element array if nulls are allowed
                    return [parse_value(value, field_type.elementType)]
                else:
                    raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
            return [parse_value(v, field_type.elementType) for v in value]
        elif isinstance(field_type, MapType):
            # Handle MapType - new support
            if not isinstance(value, dict):
                raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
            return {
                parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
                for k, v in value.items()
            }
        # Handle primitive types with more robust error handling and type conversion
        try:
            if isinstance(field_type, StringType):
                # Don't convert None to "None" string
                return str(value) if value is not None else None
            elif isinstance(field_type, (IntegerType, LongType)):
                # Convert numeric strings and floats to integers
                if isinstance(value, str) and value.strip():
                    # Handle numeric strings
                    if "." in value:
                        return int(float(value))
                    return int(value)
                elif isinstance(value, (int, float)):
                    return int(value)
                raise ValueError(f"Cannot convert {value} to integer")
            elif isinstance(field_type, FloatType) or isinstance(field_type, DoubleType):
                # New support for floating point types
                if isinstance(value, str) and value.strip():
                    return float(value)
                return float(value)
            elif isinstance(field_type, DecimalType):
                # New support for Decimal type

                if isinstance(value, str) and value.strip():
                    return Decimal(value)
                return Decimal(str(value))
            elif isinstance(field_type, BooleanType):
                # Enhanced boolean conversion
                if isinstance(value, str):
                    lowered = value.lower()
                    if lowered in ("true", "t", "yes", "y", "1"):
                        return True
                    elif lowered in ("false", "f", "no", "n", "0"):
                        return False
                return bool(value)
            elif isinstance(field_type, DateType):
                # New support for DateType
                if isinstance(value, str):
                    # Try multiple date formats
                    for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                        try:
                            return datetime.strptime(value, fmt).date()
                        except ValueError:
                            continue
                    # ISO format as fallback
                    return datetime.fromisoformat(value).date()
                elif isinstance(value, datetime):
                    return value.date()
                raise ValueError(f"Cannot convert {value} to date")
            elif isinstance(field_type, TimestampType):
                # Enhanced timestamp handling
                if isinstance(value, str):
                    # Handle multiple timestamp formats including Z and timezone offsets
                    if value.endswith("Z"):
                        value = value.replace("Z", "+00:00")
                    try:
                        return datetime.fromisoformat(value)
                    except ValueError:
                        # Try additional formats if ISO format fails
                        for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                            try:
                                return datetime.strptime(value, fmt)
                            except ValueError:
                                continue
                elif isinstance(value, (int, float)):
                    # Handle Unix timestamps
                    return datetime.fromtimestamp(value)
                elif isinstance(value, datetime):
                    return value
                raise ValueError(f"Cannot convert {value} to timestamp")
            else:
                # Check for custom UDT handling
                if hasattr(field_type, "fromJson"):
                    # Support for User Defined Types that implement fromJson
                    return field_type.fromJson(value)
                raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            # Add context to the error
            raise ValueError(
                f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}"
            )


    ########################################################
    # sources/airtable/airtable.py
    ########################################################

    class LakeflowConnect:
        """
        Airtable connector for Lakeflow.

        Connects to an Airtable base and reads tables (records) using the Airtable REST API.
        Uses Personal Access Token (PAT) for authentication.
        """

        # Mapping from Airtable field types to PySpark types
        FIELD_TYPE_MAPPING = {
            # Text fields
            "singleLineText": StringType(),
            "multilineText": StringType(),
            "richText": StringType(),
            "email": StringType(),
            "url": StringType(),
            "phoneNumber": StringType(),
            # Numeric fields
            "number": DoubleType(),
            "currency": DoubleType(),
            "percent": DoubleType(),
            "duration": LongType(),
            "rating": LongType(),
            "count": LongType(),
            "autoNumber": LongType(),
            # Boolean
            "checkbox": BooleanType(),
            # Date/Time
            "date": StringType(),  # ISO 8601 date string
            "dateTime": StringType(),  # ISO 8601 datetime string
            "createdTime": StringType(),  # System field
            "lastModifiedTime": StringType(),  # User-configured field
            # Select fields
            "singleSelect": StringType(),
            "multipleSelects": ArrayType(StringType()),
            # Links and lookups
            "multipleRecordLinks": ArrayType(StringType()),
            "lookup": ArrayType(StringType()),
            "multipleLookupValues": ArrayType(StringType()),
            # Computed fields
            "formula": StringType(),  # Result type varies, store as string
            "rollup": StringType(),  # Result type varies, store as string
            # External sync
            "externalSyncSource": StringType(),
        }

        def __init__(self, options: Dict[str, str]) -> None:
            """
            Initialize the Airtable connector with API credentials.

            Args:
                options: Dictionary containing:
                    - personal_access_token: Airtable Personal Access Token
                    - base_id: The Airtable base ID (e.g., 'appxxxxxxxxxxxxxxx')
            """
            self.personal_access_token = options["personal_access_token"]
            self.base_id = options["base_id"]
            self.base_url = "https://api.airtable.com/v0"
            self.meta_url = "https://api.airtable.com/v0/meta"

            self._headers = {
                "Authorization": f"Bearer {self.personal_access_token}",
                "Content-Type": "application/json",
            }

            # Cache for table schemas
            self._table_schemas_cache: Optional[Dict[str, Any]] = None
            self._tables_cache: Optional[List[Dict[str, Any]]] = None

        def _get_tables_metadata(self) -> List[Dict[str, Any]]:
            """
            Fetch table metadata from Airtable Metadata API.

            Returns:
                List of table metadata dictionaries including fields.
            """
            if self._tables_cache is not None:
                return self._tables_cache

            url = f"{self.meta_url}/bases/{self.base_id}/tables"
            response = requests.get(url, headers=self._headers)

            if response.status_code != 200:
                raise Exception(
                    f"Airtable API error fetching tables: {response.status_code} {response.text}"
                )

            data = response.json()
            self._tables_cache = data.get("tables", [])
            return self._tables_cache

        def _get_table_metadata(self, table_name: str) -> Dict[str, Any]:
            """
            Get metadata for a specific table by name or ID.

            Args:
                table_name: Table name or table ID

            Returns:
                Table metadata dictionary
            """
            tables = self._get_tables_metadata()

            for table in tables:
                if table.get("name") == table_name or table.get("id") == table_name:
                    return table

            raise ValueError(
                f"Table '{table_name}' not found in base '{self.base_id}'. "
                f"Available tables: {[t.get('name') for t in tables]}"
            )

        def _build_collaborator_schema(self) -> StructType:
            """Build schema for collaborator fields."""
            return StructType([
                StructField("id", StringType(), True),
                StructField("email", StringType(), True),
                StructField("name", StringType(), True),
            ])

        def _build_attachment_schema(self) -> StructType:
            """Build schema for attachment fields."""
            return StructType([
                StructField("id", StringType(), True),
                StructField("url", StringType(), True),
                StructField("filename", StringType(), True),
                StructField("size", LongType(), True),
                StructField("type", StringType(), True),
                StructField("width", LongType(), True),
                StructField("height", LongType(), True),
                StructField("thumbnails", StringType(), True),  # JSON string for nested thumbnails
            ])

        def _build_barcode_schema(self) -> StructType:
            """Build schema for barcode fields."""
            return StructType([
                StructField("text", StringType(), True),
                StructField("type", StringType(), True),
            ])

        def _get_spark_type_for_field(self, field: Dict[str, Any]) -> Any:
            """
            Convert an Airtable field definition to a PySpark data type.

            Args:
                field: Field definition from Airtable schema

            Returns:
                PySpark DataType
            """
            field_type = field.get("type", "singleLineText")

            # Handle collaborator fields
            if field_type in ["singleCollaborator", "createdBy", "lastModifiedBy"]:
                return self._build_collaborator_schema()

            if field_type == "multipleCollaborators":
                return ArrayType(self._build_collaborator_schema())

            # Handle attachments
            if field_type == "multipleAttachments":
                return ArrayType(self._build_attachment_schema())

            # Handle barcode
            if field_type == "barcode":
                return self._build_barcode_schema()

            # Handle button (typically not synced, but include for completeness)
            if field_type == "button":
                return StructType([
                    StructField("label", StringType(), True),
                    StructField("url", StringType(), True),
                ])

            # Use mapping for standard types
            return self.FIELD_TYPE_MAPPING.get(field_type, StringType())

        def list_tables(self) -> List[str]:
            """
            List available tables in the Airtable base.

            Returns:
                List of table names
            """
            tables = self._get_tables_metadata()
            return [table.get("name") for table in tables if table.get("name")]

        def get_table_schema(
            self, table_name: str, table_options: Dict[str, str]
        ) -> StructType:
            """
            Get the Spark schema for an Airtable table.

            Args:
                table_name: Name or ID of the table
                table_options: Additional options (not used currently)

            Returns:
                StructType representing the table schema
            """
            # Validate table exists
            table_metadata = self._get_table_metadata(table_name)

            fields = table_metadata.get("fields", [])

            # Build schema from Airtable fields
            schema_fields = [
                # System fields always present
                StructField("id", StringType(), False),  # Primary key, never null
                StructField("createdTime", StringType(), True),
            ]

            # Add user-defined fields
            for field in fields:
                field_name = field.get("name")
                if field_name:
                    spark_type = self._get_spark_type_for_field(field)
                    schema_fields.append(
                        StructField(field_name, spark_type, True)
                    )

            return StructType(schema_fields)

        def read_table_metadata(
            self, table_name: str, table_options: Dict[str, str]
        ) -> Dict[str, Any]:
            """
            Get metadata for an Airtable table.

            Args:
                table_name: Name or ID of the table
                table_options: Additional options (not used currently)

            Returns:
                Dictionary with primary_keys and ingestion_type
            """
            # Validate table exists
            self._get_table_metadata(table_name)

            # Airtable uses snapshot ingestion as there's no native incremental support
            return {
                "primary_keys": ["id"],
                "ingestion_type": "snapshot",
            }

        def read_table(
            self, table_name: str, start_offset: Dict, table_options: Dict[str, str]
        ) -> Tuple[Iterator[Dict], Dict]:
            """
            Read records from an Airtable table.

            Args:
                table_name: Name or ID of the table to read
                start_offset: Offset to start reading from (not used for snapshot)
                table_options: Additional options:
                    - view: Optional view name to filter/sort records
                    - fields: Optional comma-separated list of field names to return
                    - filter_by_formula: Optional Airtable formula for filtering

            Returns:
                Tuple of (records iterator, new offset)
            """
            # Validate table exists
            table_metadata = self._get_table_metadata(table_name)
            table_id_or_name = table_metadata.get("id", table_name)

            # Build query parameters
            params = {
                "pageSize": 100,  # Max allowed by Airtable
            }

            # Add optional parameters from table_options
            if table_options.get("view"):
                params["view"] = table_options["view"]

            if table_options.get("fields"):
                # fields should be a comma-separated string
                field_names = table_options["fields"].split(",")
                for field_name in field_names:
                    params.setdefault("fields[]", []).append(field_name.strip())

            if table_options.get("filter_by_formula"):
                params["filterByFormula"] = table_options["filter_by_formula"]

            # Return iterator and empty offset (snapshot mode)
            return self._read_records_iterator(table_id_or_name, params), {}

        def _read_records_iterator(
            self, table_id_or_name: str, params: Dict[str, Any]
        ) -> Iterator[Dict]:
            """
            Generator that yields records from Airtable with pagination.

            Args:
                table_id_or_name: Table ID or name
                params: Query parameters

            Yields:
                Record dictionaries
            """
            url = f"{self.base_url}/{self.base_id}/{table_id_or_name}"
            offset = None

            while True:
                # Add offset for pagination
                request_params = params.copy()
                if offset:
                    request_params["offset"] = offset

                # Make API request with rate limit handling
                response = self._make_request_with_retry(url, request_params)

                data = response.json()
                records = data.get("records", [])

                # Yield each record with flattened structure
                for record in records:
                    yield self._transform_record(record)

                # Check for more pages
                offset = data.get("offset")
                if not offset:
                    break

        def _make_request_with_retry(
            self, url: str, params: Dict[str, Any], max_retries: int = 5
        ) -> requests.Response:
            """
            Make a request with exponential backoff for rate limiting.

            Args:
                url: Request URL
                params: Query parameters
                max_retries: Maximum number of retries

            Returns:
                Response object
            """
            for attempt in range(max_retries):
                response = requests.get(url, headers=self._headers, params=params)

                if response.status_code == 200:
                    return response

                if response.status_code == 429:
                    # Rate limited - use Retry-After header or exponential backoff
                    retry_after = response.headers.get("Retry-After")
                    if retry_after:
                        wait_time = int(retry_after)
                    else:
                        wait_time = (2 ** attempt) + (0.1 * attempt)  # Exponential backoff

                    time.sleep(wait_time)
                    continue

                # Other errors
                raise Exception(
                    f"Airtable API error: {response.status_code} {response.text}"
                )

            raise Exception(f"Max retries exceeded for Airtable API request to {url}")

        def _transform_record(self, record: Dict[str, Any]) -> Dict[str, Any]:
            """
            Transform an Airtable record to a flat structure.

            Airtable returns records as:
            {
                "id": "recXXX",
                "createdTime": "2024-01-01T00:00:00.000Z",
                "fields": {"field1": "value1", ...}
            }

            This transforms to:
            {
                "id": "recXXX",
                "createdTime": "2024-01-01T00:00:00.000Z",
                "field1": "value1",
                ...
            }

            Args:
                record: Raw Airtable record

            Returns:
                Transformed record dictionary
            """
            result = {
                "id": record.get("id"),
                "createdTime": record.get("createdTime"),
            }

            # Flatten fields into the result
            fields = record.get("fields", {})
            for field_name, field_value in fields.items():
                result[field_name] = field_value

            return result

        def test_connection(self) -> Dict[str, str]:
            """
            Test the connection to Airtable API.

            Returns:
                Dictionary with status and message
            """
            try:
                # Try to list bases to verify token works
                url = f"{self.meta_url}/bases"
                response = requests.get(url, headers=self._headers)

                if response.status_code == 200:
                    return {"status": "success", "message": "Connection successful"}
                else:
                    return {
                        "status": "error",
                        "message": f"API error: {response.status_code} {response.text}",
                    }
            except Exception as e:
                return {"status": "error", "message": f"Connection failed: {str(e)}"}


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            records, offset = self.lakeflow_connect.read_table(
                self.options["tableName"], start, self.options
            )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(table, self.options)
                all_records.append({"tableName": table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options["tableName"]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField("tableName", StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)
